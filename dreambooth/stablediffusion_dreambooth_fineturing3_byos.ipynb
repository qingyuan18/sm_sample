{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed034c1a-156a-4beb-9a38-af2894187994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi\n",
    "#!dpkg --print-architecture\n",
    "#!gcc -v\n",
    "#!cat /etc/issue\n",
    "!pip3 list|grep -i torch\n",
    "!pip3 list|grep -i cud\n",
    "!pip list|grep -i xformer\n",
    "#!python -m torch.utils.collect_env\n",
    "#!pip3 list|grep -i triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639cab2-e673-49c3-bac9-4e9627747172",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./training && git clone https://github.com/qingyuan18/sd_dreambooth_extension.git ./extensions/sd_dreambooth_extension/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5d0d3-a2f6-4911-9aa9-2f3c14fa7a39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r ./training/extensions/sd_dreambooth_extension/requirements.txt\n",
    "#!pip install -r ./training/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02d246-7a87-4bac-a609-9fbd2c4b7e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! apt-get update\n",
    "! apt-get install --assume-yes apt-utils -y\n",
    "\n",
    "! apt update\n",
    "! echo \"Y\"|apt install vim\n",
    "! apt install wget git -y\n",
    "! apt install libgl1-mesa-glx -y\n",
    "! pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab0ef5-8fd9-47b7-82db-f3f3c3980eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install ninja triton==2.0.0.dev20221120 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba707ec1-36f5-4683-9a0b-c9e6991f5aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.6\" && export FORCE_CUDA=\"1\"&&git clone https://github.com/xieyongliang/xformers.git ./repositories/xformers && cd ./repositories/xformers && git submodule update --init --recursive && pip install -r requirements.txt && pip install -e . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c14770-6b8a-41b8-9e94-e9321c2adefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"Y\"|pip uninstall xformers==0.0.16rc425\n",
    "!echo \"Y\"|pip uninstall xformers==0.0.16.dev426\n",
    "#!pip install xformers==0.0.16.dev426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279d922-766c-4a71-8ed4-363467c9c817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching transformers to fix kwargs errors.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "{'model_name': 'aws-trained-dreambooth-model', 'models_path': './model/', 'use_lora': False, 'use_cpu': False, 'lora_models_path': None, 'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-2', 'pretrained_vae_name_or_path': None, 'revision': None, 'tokenizer_name': None, 'instance_data_dir': './images/', 'class_data_dir': './images/', 'instance_prompt': 'Erwin Rommel', 'class_prompt': 'a photo of Erwin Rommel', 'pad_tokens': False, 'with_prior_preservation': True, 'save_use_global_counts': False, 'save_use_epochs': True, 'prior_loss_weight': 0.5, 'num_class_images': 0, 'output_dir': 'text-inversion-model', 'seed': -1, 'resolution': 512, 'center_crop': False, 'train_text_encoder': 'True', 'train_batch_size': 1, 'sample_batch_size': 1, 'num_train_epochs': 1, 'max_train_steps': 600, 'epoch': 0, 'save_steps': 600, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': True, 'learning_rate': 5e-06, 'scale_lr': False, 'lr_scheduler': 'constant', 'lr_warmup_steps': 100, 'use_8bit_adam': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'push_to_hub': False, 'hub_token': None, 'hub_model_id': None, 'logging_dir': 'logs', 'mixed_precision': 'fp16', 'not_cache_latents': 'True', 'hflip': False, 'local_rank': -1, 'concepts_list': [{'instance_prompt': 'Erwin Rommel', 'class_prompt': 'a photo of Erwin Rommel', 'instance_data_dir': './images/', 'class_data_dir': './images/', 'num_class_images': 0, 'instance_token': '', 'class_token': '', 'class_negative_prompt': '', 'class_guidance_scale': 7.5, 'class_infer_steps': 60}], 'use_ema': True, 'max_token_length': 75, 'half_model': False, 'attention': 'xformers', 'shuffle_tags': False}\n",
      "Replace CrossAttention.forward to use xformers\n",
      "Checking concept: {'instance_prompt': 'Erwin Rommel', 'class_prompt': 'a photo of Erwin Rommel', 'instance_data_dir': './images/', 'class_data_dir': './images/', 'num_class_images': 0, 'instance_token': '', 'class_token': '', 'class_negative_prompt': '', 'class_guidance_scale': 7.5, 'class_infer_steps': 60}\n",
      "Concept requires 0 images.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      " Loaded model. \n",
      " Allocated: 0.0GB \n",
      " Reserved: 0.0GB \n",
      "\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
      "  warn(\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/_sagemaker-instance-credentials/630a4e3c71ad8bf37980d16f5cb7edce24a7d7b1d08a07e4c954a9266cd8c0dc')}\n",
      "  warn(\n",
      "/opt/conda/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('$(dirname $(which conda))/..')}\n",
      "  warn(\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      " Scheduler, EMA Loaded. \n",
      " Allocated: 6.4GB \n",
      " Reserved: 6.4GB \n",
      "\n",
      "***** Running training *****\n",
      "  Num examples = 5\n",
      "  Num batches each epoch = 5\n",
      "  Num Epochs = 120\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "  Actual steps: 600\n",
      "   Training settings: CPU: False Adam: True, Prec: fp16, Grad: True, TextTr: True EM: True, LR: 5e-06 LORA:False \n",
      " Allocated: 6.4GB \n",
      " Reserved: 6.4GB \n",
      "\n",
      "Steps:   0%|                                            | 0/600 [00:00<?, ?it/s][2023-01-30 12:08:24.258: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.25.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-01-30 12:08:24.304 pytorch-1-12-gpu-py-ml-g4dn-xlarge-ee04a2c19cee62cf2e77e4a42e46:4371 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[2023-01-30 12:08:24.443 pytorch-1-12-gpu-py-ml-g4dn-xlarge-ee04a2c19cee62cf2e77e4a42e46:4371 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "You're using a CLIPTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Steps:   0%|       | 0/600 [00:04<?, ?it/s, loss=0.418, lr=5e-6, vram=8.7/9.3GB] Step 0 completed. \n",
      " Allocated: 8.7GB \n",
      " Reserved: 9.3GB \n",
      "\n",
      "Steps:   1%| | 5/600 [00:09<14:34,  1.47s/it, loss=0.387, lr=5e-6, vram=8.7/9.3G\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 2485.57it/s]\u001b[A\n",
      " Step 5 completed. \n",
      " Allocated: 8.7GB \n",
      " Reserved: 9.3GB \n",
      "\n",
      "Steps:   7%| | 40/600 [07:07<54:13,  5.81s/it, loss=0.37, lr=5e-6, vram=8.7/9.3G"
     ]
    }
   ],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF='max_split_size_mb:32'&& python ./training/train.py \\\n",
    "--attention xformers  \\\n",
    "--class_data_dir \"./images/\"  \\\n",
    "--class_prompt \"a photo of Erwin Rommel\" \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--gradient_checkpointing True \\\n",
    "--instance_data_dir \"./images/\" \\\n",
    "--instance_prompt \"Erwin Rommel\" \\\n",
    "--learning_rate 5e-06 \\\n",
    "--lr_scheduler constant  \\\n",
    "--lr_warmup_steps 100  \\\n",
    "--max_train_steps 600  \\\n",
    "--mixed_precision fp16  \\\n",
    "--model_name aws-trained-dreambooth-model \\\n",
    "--models_path \"./model/\"  \\\n",
    "--not_cache_latents True  \\\n",
    "--num_class_images 0   \\\n",
    "--pretrained_model_name_or_path \"stabilityai/stable-diffusion-2\" \\\n",
    "--prior_loss_weight 0.5 \\\n",
    "--resolution 512  \\\n",
    "--sample_batch_size 1 \\\n",
    "--save_steps 600  \\\n",
    "--train_batch_size 1 \\\n",
    "--train_text_encoder True \\\n",
    "--use_ema True \\\n",
    "--with_prior_preservation True \\\n",
    "--save_use_epochs False \\\n",
    "--use_8bit_adam True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f010944f-e0fe-472a-84c5-6e4609c36b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # 查看torch当前版本号\n",
    "print(torch.version.cuda)  # 编译当前版本的torch使用的cuda版本号\n",
    "print(torch.cuda.is_available())  # 查看当前cuda是否可用于当前版本的Torch，如果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac85d9e-e504-4d3f-828f-e0bf94bfa4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
