{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Standard instruction for using LMI container on SageMaker\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers, multidatamodel\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64070970-025a-4887-b929-8adfb44ab1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "diffusers==0.12.0\n",
    "ftfy\n",
    "boto3\n",
    "spacy\n",
    "omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "87b9e0a5-eb8d-4d9e-8f30-b0d36f07ec40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "\n",
    "import requests\n",
    "import boto3\n",
    "import torch\n",
    "import os\n",
    "import sagamaker\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from djl_python import Input, Output\n",
    "from diffusers import StableDiffusionPipeline,StableDiffusionImg2ImgPipeline\n",
    "from diffusers import EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, HeunDiscreteScheduler, LMSDiscreteScheduler, KDPM2DiscreteScheduler, KDPM2AncestralDiscreteScheduler,DDIMScheduler\n",
    "\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def get_default_bucket():\n",
    "    try:\n",
    "        sagemaker_session = sagemaker.Session() if custom_region is None else sagemaker.Session(\n",
    "            boto3.Session(region_name=custom_region))\n",
    "        bucket = sagemaker_session.default_bucket()\n",
    "        return bucket\n",
    "    except Exception as ex:\n",
    "        if s3_bucket!=\"\":\n",
    "            return s3_bucket\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def get_bucket_and_key(s3uri):\n",
    "    pos = s3uri.find('/', 5)\n",
    "    bucket = s3uri[5 : pos]\n",
    "    key = s3uri[pos + 1 : ]\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def init_normal_model(model_name,model_args):\n",
    "    model_path=model_name\n",
    "    base_name=os.path.basename(model_name)\n",
    "    try:\n",
    "        if model_name.startswith(\"s3://\"):\n",
    "            if base_name==\"model.tar.gz\":\n",
    "                local_path= \"/\".join(model_name.split(\"/\")[-2:-1])\n",
    "                model_path=f\"/tmp/{local_path}\"\n",
    "                print(f\"need copy {model_name} to {model_path}\")\n",
    "                print(\"downloading model from s3:\", model_name)\n",
    "                if not os.path.exists(model_path):\n",
    "                    os.makedirs(model_path)\n",
    "                #download trained model from model_path(s3uri)\n",
    "                download_model(model_name,model_path+\"/model.tar.gz\")\n",
    "                print(\"model download to target path:\", model_path)\n",
    "                #extract model.tar.gz in /tmp/models folder\n",
    "                model_file = tarfile.open(model_path+\"/model.tar.gz\")\n",
    "                model_file.extractall(model_path)\n",
    "                print('model extracted: ', os.listdir(model_path))\n",
    "                model_file.close()\n",
    "                os.remove(model_path+\"/model.tar.gz\")\n",
    "                #fs.get(model_name,model_path+\"/\", recursive=True)\n",
    "                #untar(f\"/tmp/{local_path}/model.tar.gz\",model_path)\n",
    "                #os.remove(f\"/tmp/{local_path}/model.tar.gz\")\n",
    "                print(\"download and untar  completed\")\n",
    "            else:\n",
    "                local_path= \"/\".join(model_name.split(\"/\")[-2:])\n",
    "                model_path=f\"/tmp/{local_path}\"\n",
    "                print(f\"need copy {model_name} to {model_path}\")\n",
    "                os.makedirs(model_path)\n",
    "                fs.get(model_name,model_path, recursive=True)\n",
    "                print(\"download completed\")\n",
    "\n",
    "        print(f\"pretrained model_path: {model_path}\")\n",
    "        if model_args is not None:\n",
    "            return StableDiffusionPipeline.from_pretrained(\n",
    "                model_path, **model_args).to(\"cuda\")\n",
    "        return StableDiffusionPipeline.from_pretrained(model_path).to(\"cuda\")\n",
    "    except Exception as ex:\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print(f\"=================Exception================={ex}\")\n",
    "        return None\n",
    "\n",
    "def get_model(properties):\n",
    "    model_name = properties['model_id']\n",
    "    print(\"here1=======\"+model_name)\n",
    "    pipeline=init_normal_model(model_name,None)\n",
    "    return pipeline\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    \n",
    "    input_data = inputs.get_as_json()\n",
    "    generator = torch.Generator(device='cuda').manual_seed(input_data[\"seed\"])\n",
    "    with autocast(\"cuda\"):\n",
    "        images = predictor(input_data[\"prompt\"], negative_prompt=input_data[\"negative_prompt\"],\n",
    "                               num_inference_steps=input_data[\"steps\"], num_images_per_prompt=input_data[\"count\"], generator=generator).images\n",
    "    prediction = []\n",
    "    for image in images:\n",
    "        bucket= get_default_bucket()\n",
    "        if bucket is None:\n",
    "            raise Exception(\"Need setup default bucket\")\n",
    "        output_s3uri = f's3://{bucket}/stablediffusion/asyncinvoke/images/'\n",
    "        bucket, key = get_bucket_and_key(output_s3uri)\n",
    "        key = f'{key}{uuid.uuid4()}.png'\n",
    "        buf = io.BytesIO()\n",
    "        image.save(buf, format='PNG')\n",
    "\n",
    "        s3_client.put_object(\n",
    "            Body=buf.getvalue(),\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            ContentType='image/jpeg',\n",
    "            Metadata={\n",
    "                # #s3 metadata only support ascii\n",
    "                \"seed\": str(input_data[\"seed\"])\n",
    "            }\n",
    "        )\n",
    "        print('image: ', f's3://{bucket}/{key}')\n",
    "        prediction.append(f's3://{bucket}/{key}')\n",
    "    result = json.dumps({'result': prediction})    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "671ea485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "models_to_run=[\"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\", \"CompVis/stable-diffusion-v1-4\"]\n",
    "model_folders = [model.split(\"/\")[1].lower() for model in models_to_run]\n",
    "\n",
    "for folder, model in zip(model_folders, models_to_run):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    with open(os.path.join(folder, \"serving.properties\"), \"w\") as f:\n",
    "        f.write(f\"engine=Python\\noption.model_id={model}\\n\")\n",
    "    shutil.copyfile(\"model.py\", f\"{folder}/model.py\")\n",
    "    shutil.copyfile(\"requirements.txt\", f\"{folder}/requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9949c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DJLServing memory management for MME\n",
    "\n",
    "In DJLServing, you could control how many memory allocated for each CPU/GPU on SageMaker. It works like below:\n",
    "\n",
    "- `required_memory_mb` CPU/GPU required memory in MB\n",
    "- `reserved_memory_mb` CPU/GPU reserved memory for computation\n",
    "- `gpu.required_memory_mb` GPU required memory in MB\n",
    "- `gpu.reserved_memory_mb` GPU reserved memory for computation\n",
    "\n",
    "If you need 20GB CPU memory and 2GB GPU memory, you could set\n",
    "\n",
    "```\n",
    "required_memory_mb=20480\n",
    "gpu.required_memory_mb=2048\n",
    "```\n",
    "\n",
    "in the following code, we will create a bomb model that plans to take over all GPU memory and let's see how that would impact the result. For more information on settings, please find them [here](https://docs.djl.ai/docs/serving/serving/docs/modes.html#servingproperties)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ae03130-065e-44be-930c-66a7059ce7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable-diffusion-v1-5/\n",
      "stable-diffusion-v1-5/model.py\n",
      "stable-diffusion-v1-5/serving.properties\n",
      "stable-diffusion-v1-5/requirements.txt\n",
      "stable-diffusion-2-1/\n",
      "stable-diffusion-2-1/model.py\n",
      "stable-diffusion-2-1/serving.properties\n",
      "stable-diffusion-2-1/requirements.txt\n",
      "stable-diffusion-v1-4/\n",
      "stable-diffusion-v1-4/model.py\n",
      "stable-diffusion-v1-4/serving.properties\n",
      "stable-diffusion-v1-4/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tar czvf stable-diffusion-v1-5.tar.gz stable-diffusion-v1-5/\n",
    "tar czvf stable-diffusion-2-1.tar.gz stable-diffusion-2-1/\n",
    "tar czvf stable-diffusion-v1-4.tar.gz stable-diffusion-v1-4/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "Available framework are:\n",
    "- djl-deepspeed (0.20.0, 0.21.0)\n",
    "- djl-fastertransformer (0.21.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.21.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-687912291502/large-model-lmi/code/stable-diffusion-v1-5.tar.gz\n",
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-687912291502/large-model-lmi/code/stable-diffusion-2-1.tar.gz\n",
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-687912291502/large-model-lmi/code/stable-diffusion-v1-4.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "for model_name in model_folders:\n",
    "    code_artifact = sess.upload_data(f\"{model_name}.tar.gz\", bucket, s3_code_prefix)\n",
    "    print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "env = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \"TRANSFORMERS_CACHE\": \"/tmp\"}\n",
    "model_s3_folder = os.path.dirname(code_artifact) + \"/\"\n",
    "\n",
    "model = multidatamodel.MultiDataModel(\"LMITestModel\", model_s3_folder, image_uri=image_uri, env=env, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8e0e61cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: LMITestModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2bcef095",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\n  \"code\":424,\n  \"message\":\"prediction failure\",\n  \"error\":\"name 'output_s3uri' is not defined\"\n}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/lmi-model-2023-05-15-10-07-34-600 in account 687912291502 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27122/3640406181.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"a happy weekend with my family\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"negative_prompt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stable-diffusion-v1-5.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"a happy weekend with my family\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"negative_prompt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stable-diffusion-2-1.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"a happy weekend with my family\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"negative_prompt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stable-diffusion-v1-4.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/base_predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         )\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\n  \"code\":424,\n  \"message\":\"prediction failure\",\n  \"error\":\"name 'output_s3uri' is not defined\"\n}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/lmi-model-2023-05-15-10-07-34-600 in account 687912291502 for more information."
     ]
    }
   ],
   "source": [
    "print(predictor.predict( {\"prompt\": \"a happy weekend with my family\",\"seed\":-1,\"negative_prompt\":\"\",\"steps\":30,\"count\":1}, target_model=\"stable-diffusion-v1-5.tar.gz\"))\n",
    "print(predictor.predict({\"prompt\": \"a happy weekend with my family\",\"seed\":-1,\"negative_prompt\":\"\",\"steps\":30,\"count\":1}, target_model=\"stable-diffusion-2-1.tar.gz\"))\n",
    "print(predictor.predict({\"prompt\": \"a happy weekend with my family\",\"seed\":-1,\"negative_prompt\":\"\",\"steps\":30,\"count\":1}, target_model=\"stable-diffusion-v1-4.tar.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bac44",
   "metadata": {},
   "source": [
    "### Testing a bomb model\n",
    "\n",
    "Now let's see if I have a model need 30GB GPU memory and what will happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_artifact = sess.upload_data(f\"bomb.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "try:\n",
    "    predictor.predict({\"prompt\": \"Large model inference is\"}, target_model=\"bomb.tar.gz\")\n",
    "except Exception as e:\n",
    "    print(\"Loading failed...You can still load more models that are smaller than the gpu sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "The model loading failed since the total GPU memory is 24GB and cannot holds a 30GB model. You will find the model server is still alive. Behind the scence, SageMaker will unload all models to spare spaces. So currently there is no model loaded. You could rerun the 4 prediction above and model server will reload the model back again.\n",
    "\n",
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}