{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579322a2",
   "metadata": {},
   "source": [
    "## Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526743e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.71 requires botocore==1.29.71, but you have botocore 1.29.94 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python -m pip install --upgrade pip\n",
    "! pip install botocore --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90adc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.116.0\" \"huggingface_hub==0.13.1\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837b2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16449ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import base64\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionPipeline,DiffusionPipeline\n",
    "import deepspeed\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    # Load stable diffusion and move it to the GPU\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16, revision=\"fp16\")\n",
    "    pipe=deepspeed.init_inference(\n",
    "        model=getattr(pipe,\"model\", pipe),      # Transformers models\n",
    "        mp_size=1,        # Number of GPU\n",
    "        dtype=torch.float16, # dtype of the weights (fp16)\n",
    "        replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "        replace_with_kernel_inject=False, # replace the model with the kernel injector\n",
    "    )\n",
    "\n",
    "    print(\"!!!!DeepSpeed Inference Engine initialized!!!!!!!!\")\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    torch.cuda.synchronize(\"cuda\")\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "\n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    print(prompt)\n",
    "    # set valid HP for stable diffusion\n",
    "    num_inference_steps = data.pop(\"num_inference_steps\", 50)\n",
    "    guidance_scale = data.pop(\"guidance_scale\", 7.5)\n",
    "    num_images_per_prompt = data.pop(\"num_images_per_prompt\", 4)\n",
    "    width = data.pop(\"width\", 512)\n",
    "    height = data.pop(\"height\", 512)\n",
    "\n",
    "    # run generation with parameters\n",
    "    generated_images = pipe(\n",
    "        prompt,\n",
    "        #num_inference_steps=num_inference_steps,\n",
    "        #guidance_scale=guidance_scale,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_images_per_prompt=num_images_per_prompt\n",
    "    )[\"images\"]\n",
    "\n",
    "    # create response\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"generated_images\": encoded_images}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3716b04-a798-435d-b81b-6479382094fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Downloading diffusers-0.14.0-py3-none-any.whl (737 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from -r ./inference/requirements.txt (line 5)) (1.26.69)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (from -r ./inference/requirements.txt (line 6)) (2.132.0)\n",
      "Collecting nvgpu\n",
      "  Using cached nvgpu-0.9.0-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (from -r ./inference/requirements.txt (line 8)) (0.4.2)\n",
      "Collecting k_diffusion\n",
      "  Downloading k_diffusion-0.0.14-py3-none-any.whl (25 kB)\n",
      "Collecting omegaconf\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting imwatermark\n",
      "  Downloading imWatermark-0.0.2-py3-none-any.whl (2.8 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting deepspeed==0.7.6\n",
      "  Downloading deepspeed-0.7.6.tar.gz (709 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.9/709.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ninja\n",
      "  Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "Collecting triton==2.0.0.dev20221120\n",
      "  Using cached triton-2.0.0.dev20221120-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.7 MB)\n",
      "Collecting deepspeed-mii==0.0.3\n",
      "  Using cached deepspeed_mii-0.0.3-py3-none-any.whl (44 kB)\n",
      "Collecting clip==0.2.0\n",
      "  Using cached clip-0.2.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.7.6->-r ./inference/requirements.txt (line 13)) (1.21.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.7.6->-r ./inference/requirements.txt (line 13)) (20.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.7.6->-r ./inference/requirements.txt (line 13)) (5.6.7)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-1.10.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.7.6->-r ./inference/requirements.txt (line 13)) (4.42.1)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.25.2-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from triton==2.0.0.dev20221120->-r ./inference/requirements.txt (line 15)) (3.0.12)\n",
      "Collecting grpcio\n",
      "  Downloading grpcio-1.51.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-tools\n",
      "  Downloading grpcio_tools-1.51.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting asyncio\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.10.0\n",
      "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from diffusers->-r ./inference/requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from diffusers->-r ./inference/requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from diffusers->-r ./inference/requirements.txt (line 1)) (9.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from diffusers->-r ./inference/requirements.txt (line 1)) (6.0.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->-r ./inference/requirements.txt (line 2)) (6.0)\n",
      "Collecting wcwidth>=0.2.5\n",
      "  Using cached wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->-r ./inference/requirements.txt (line 4)) (59.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r ./inference/requirements.txt (line 4)) (3.1.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (912 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.1/912.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy->-r ./inference/requirements.txt (line 4)) (4.4.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->-r ./inference/requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.69 in /opt/conda/lib/python3.7/site-packages (from boto3->-r ./inference/requirements.txt (line 5)) (1.29.69)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->-r ./inference/requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (0.1.5)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (0.7.5)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r ./inference/requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: ansi2html in /opt/conda/lib/python3.7/site-packages (from nvgpu->-r ./inference/requirements.txt (line 7)) (1.8.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nvgpu->-r ./inference/requirements.txt (line 7)) (1.14.0)\n",
      "Collecting flask-restful\n",
      "  Using cached Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: flask in /opt/conda/lib/python3.7/site-packages (from nvgpu->-r ./inference/requirements.txt (line 7)) (1.1.1)\n",
      "Collecting pynvml\n",
      "  Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from nvgpu->-r ./inference/requirements.txt (line 7)) (0.9.0)\n",
      "Collecting arrow\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from s3fs->-r ./inference/requirements.txt (line 8)) (2023.1.0)\n",
      "Collecting kornia\n",
      "  Downloading kornia-0.6.10-py2.py3-none-any.whl (612 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.0/612.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting resize-right\n",
      "  Using cached resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "Collecting torchdiffeq\n",
      "  Using cached torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
      "Collecting clean-fid\n",
      "  Using cached clean_fid-0.1.35-py3-none-any.whl (26 kB)\n",
      "Collecting torchsde\n",
      "  Using cached torchsde-0.2.5-py3-none-any.whl (59 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting clip-anytorch\n",
      "  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from k_diffusion->-r ./inference/requirements.txt (line 9)) (1.4.1)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from k_diffusion->-r ./inference/requirements.txt (line 9)) (0.16.2)\n",
      "Collecting einops\n",
      "  Using cached einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "Collecting jsonmerge\n",
      "  Using cached jsonmerge-1.9.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imhist\n",
      "  Downloading imhist-0.0.4-py3-none-any.whl (2.8 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.69->boto3->-r ./inference/requirements.txt (line 5)) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.69->boto3->-r ./inference/requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy->-r ./inference/requirements.txt (line 4)) (3.13.0)\n",
      "Collecting packaging\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers->-r ./inference/requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers->-r ./inference/requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers->-r ./inference/requirements.txt (line 1)) (2022.12.7)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->deepspeed==0.7.6->-r ./inference/requirements.txt (line 13)) (0.38.4)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from flask->nvgpu->-r ./inference/requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from flask->nvgpu->-r ./inference/requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy->-r ./inference/requirements.txt (line 4)) (2.1.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from flask-restful->nvgpu->-r ./inference/requirements.txt (line 7)) (2019.3)\n",
      "Collecting aniso8601>=0.82\n",
      "  Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "Collecting grpcio-tools\n",
      "  Downloading grpcio_tools-1.51.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading grpcio_tools-1.50.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading grpcio_tools-1.49.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading grpcio_tools-1.48.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>2.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonmerge->k_diffusion->-r ./inference/requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker->-r ./inference/requirements.txt (line 6)) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker->-r ./inference/requirements.txt (line 6)) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker->-r ./inference/requirements.txt (line 6)) (0.3.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker->-r ./inference/requirements.txt (line 6)) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker->-r ./inference/requirements.txt (line 6)) (0.6.0.post1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (2.6.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (3.1.3)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (2.4)\n",
      "Collecting trampoline>=0.1.2\n",
      "  Using cached trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "Collecting boltons>=20.2.1\n",
      "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb->k_diffusion->-r ./inference/requirements.txt (line 9)) (0.1.2)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>2.4.0->jsonmerge->k_diffusion->-r ./inference/requirements.txt (line 9)) (0.15.7)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->k_diffusion->-r ./inference/requirements.txt (line 9)) (4.4.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: deepspeed, clip, antlr4-python3-runtime, jsonmerge\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.7.6-py3-none-any.whl size=714379 sha256=cd943c3c7af946fe30cfbdf501f326e2113659699d5cc40611c1a689ea7b779e\n",
      "  Stored in directory: /root/.cache/pip/wheels/00/f2/51/bc76882ae7693ea88a1b52d9bc07aedea3f57fc0b2ddde6d7a\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=7005 sha256=c34fb4a9eb45c98846f7b20ec096bebb1fcf2da1900dbbe090315c863045e239\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/09/22/2a08ba22ac0a9939e09ca9651cbc83fd781894f4b24df79a1b\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=8c8120dd133b3f6c35cc4e9c5b6123dff29991c55c6e719a37fd3d66d20ed9c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
      "  Building wheel for jsonmerge (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonmerge: filename=jsonmerge-1.9.0-py3-none-any.whl size=18633 sha256=5e5f364697c24636da918b38dbdcc778955b02c07cd12bfa03b8cd0aa8272a88\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/26/18/ced36f43cdba1691e8b657bffce76353cd97329301c3aa9fea\n",
      "Successfully built deepspeed clip antlr4-python3-runtime jsonmerge\n",
      "Installing collected packages: wcwidth, trampoline, tokenizers, sentencepiece, resize-right, py-cpuinfo, ninja, hjson, cymem, cmake, clip, boltons, asyncio, appdirs, antlr4-python3-runtime, aniso8601, wasabi, termcolor, spacy-loggers, spacy-legacy, smmap, smart-open, setproctitle, sentry-sdk, scipy, pynvml, pydantic, packaging, omegaconf, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, murmurhash, langcodes, importlib-metadata, imhist, grpcio, ftfy, einops, docker-pycreds, catalogue, blis, srsly, preshed, nvidia-cudnn-cu11, imwatermark, huggingface-hub, grpcio-tools, gitdb, click, arrow, typer, transformers, torch, jsonmerge, GitPython, diffusers, confection, wandb, triton, torchvision, torchsde, torchdiffeq, thinc, pathy, kornia, flask-restful, deepspeed, accelerate, spacy, nvgpu, deepspeed-mii, clip-anytorch, clean-fid, k_diffusion\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.1.8\n",
      "    Uninstalling wcwidth-0.1.8:\n",
      "      Successfully uninstalled wcwidth-0.1.8\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: Click 7.0\n",
      "    Uninstalling Click-7.0:\n",
      "      Successfully uninstalled Click-7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed GitPython-3.1.31 accelerate-0.16.0 aniso8601-9.0.1 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 arrow-1.2.3 asyncio-3.4.3 blis-0.7.9 boltons-23.0.0 catalogue-2.0.8 clean-fid-0.1.35 click-8.1.3 clip-0.2.0 clip-anytorch-2.5.2 cmake-3.25.2 confection-0.0.4 cymem-2.0.7 deepspeed-0.7.6 deepspeed-mii-0.0.3 diffusers-0.14.0 docker-pycreds-0.4.0 einops-0.6.0 flask-restful-0.3.9 ftfy-6.1.1 gitdb-4.0.10 grpcio-1.51.3 grpcio-tools-1.48.2 hjson-3.1.0 huggingface-hub-0.12.1 imhist-0.0.4 importlib-metadata-4.13.0 imwatermark-0.0.2 jsonmerge-1.9.0 k_diffusion-0.0.14 kornia-0.6.10 langcodes-3.3.0 murmurhash-1.0.9 ninja-1.11.1 nvgpu-0.9.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 omegaconf-2.3.0 packaging-23.0 pathy-0.10.1 preshed-3.0.8 py-cpuinfo-9.0.0 pydantic-1.10.5 pynvml-11.5.0 resize-right-0.0.2 scipy-1.7.3 sentencepiece-0.1.97 sentry-sdk-1.16.0 setproctitle-1.3.2 smart-open-6.3.0 smmap-5.0.0 spacy-3.5.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 termcolor-2.2.0 thinc-8.1.9 tokenizers-0.13.2 torch-1.13.1 torchdiffeq-0.2.3 torchsde-0.2.5 torchvision-0.14.1 trampoline-0.1.2 transformers-4.26.1 triton-2.0.0.dev20221120 typer-0.7.0 wandb-0.13.11 wasabi-1.1.1 wcwidth-0.2.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./inference/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0c46a53-061b-4ca1-8a68-a1f752a6e615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded: StableDiffusionPipeline {\n",
      "  \"_class_name\": \"StableDiffusionPipeline\",\n",
      "  \"_diffusers_version\": \"0.14.0\",\n",
      "  \"feature_extractor\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"requires_safety_checker\": false,\n",
      "  \"safety_checker\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"PNDMScheduler\"\n",
      "  ],\n",
      "  \"text_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"tokenizer\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNet2DConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKL\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "begin load deepspeed....\n",
      "[2023-03-08 02:00:33,784] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.7.6, git-hash=unknown, git-branch=unknown\n",
      "[2023-03-08 02:00:33,785] [WARNING] [config_utils.py:68:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-03-08 02:00:33,786] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "deepspeed accelarate excpetion!\n",
      "module 'diffusers.models.vae' has no attribute 'AutoencoderKL'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import boto3\n",
    "import sagemaker\n",
    "import uuid\n",
    "import torch\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import traceback\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import deepspeed\n",
    "\n",
    "\n",
    "\n",
    "#model_dir='/root/dreambooth/models_diffuser/'\n",
    "model_dir='runwayml/stable-diffusion-v1-5'\n",
    "model = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16, revision=\"fp16\")\n",
    "print(\"model loaded:\",model)\n",
    " \n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"begin load deepspeed....\")    \n",
    "    model=deepspeed.init_inference(\n",
    "        model=getattr(model,\"model\", model),      # Transformers models\n",
    "        mp_size=1,        # Number of GPU\n",
    "        dtype=torch.float16, # dtype of the weights (fp16)\n",
    "        replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "        replace_with_kernel_inject=False, # replace the model with the kernel injector\n",
    "    )\n",
    "    print('model accelarate with deepspeed!')\n",
    "except Exception as e:\n",
    "    print(\"deepspeed accelarate excpetion!\")\n",
    "    print(e)\n",
    "    \n",
    "model = model.to(\"cuda\")\n",
    "model.enable_attention_slicing()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef7462",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988518bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "  base64_image = base64.b64decode(image_string)\n",
    "  buffer = BytesIO(base64_image)\n",
    "  return Image.open(buffer)\n",
    "\n",
    "# display PIL images as grid\n",
    "def display_images(images=None,columns=3, width=100, height=100):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = \"A dog trying catch a flying pizza art drawn by disney concept artists, golden colour, high quality, highly detailed, elegant, sharp focus\"\n",
    "#prompt = \"portrait photo headshot by mucha, sharp focus, elegant, render, octane, detailed, award winning photography, masterpiece, rim lit\"\n",
    "#prompt = \"priest, blue robes, 68 year old nun, national geographic, portrait, photo, photography\"\n",
    "#prompt = \"hotel room with a swimming pool outside of the window, TV on the table, moon in the sky\"\n",
    "\n",
    "\n",
    "images = model(prompt, num_images_per_prompt=5, num_inference_steps=25, guidance_scale=9).images\n",
    "             for image in images:\n",
    "                bucket, key = get_bucket_and_key(output_s3uri)\n",
    "                key = '{0}{1}.jpg'.format(key, uuid.uuid4())\n",
    "                buf = io.BytesIO()\n",
    "                image.save(buf, format='JPEG')\n",
    "                s3_client.put_object(\n",
    "                    Body = buf.getvalue(), \n",
    "                    Bucket = bucket, \n",
    "                    Key = key, \n",
    "                    ContentType = 'image/jpeg'\n",
    "                )\n",
    "                print('image: ', 's3://{0}/{1}'.format(bucket, key))\n",
    "\n",
    "# decode images\n",
    "decoded_images = [decode_base64_image(image) for image in response[\"generated_images\"]]\n",
    "\n",
    "# visualize generation\n",
    "display_images(decoded_images)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
