{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579322a2",
   "metadata": {},
   "source": [
    "## Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526743e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (23.0)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0\n",
      "    Uninstalling pip-23.0:\n",
      "      Successfully uninstalled pip-23.0\n",
      "Successfully installed pip-23.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.70 requires botocore==1.29.70, but you have botocore 1.29.86 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python -m pip install --upgrade pip\n",
    "! pip install botocore --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90adc7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.26.1 requires huggingface-hub<1.0,>=0.11.0, but you have huggingface-hub 0.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker==2.116.0\" \"huggingface_hub==0.10.1\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837b2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20220807T143615\n",
      "sagemaker bucket: sagemaker-ap-southeast-1-687912291502\n",
      "sagemaker session region: ap-southeast-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36205032",
   "metadata": {},
   "source": [
    "### Prepare BYOS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6709a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f1ef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "sentencepiece==0.1.97\n",
    "accelerate==0.14.0\n",
    "diffusers==0.9.0\n",
    "transformers==4.24.0\n",
    "huggingface-hub==0.11.1\n",
    "tokenizers==0.12.1\n",
    "ftfy==6.1.1\n",
    "deepspeed==0.7.4\n",
    "deepspeed-mii==0.0.3\n",
    "triton==2.0.0.dev20221005\n",
    "clip==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16449ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import base64\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionPipeline,DiffusionPipeline\n",
    "import deepspeed\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    # Load stable diffusion and move it to the GPU\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16, revision=\"fp16\")\n",
    "    pipe=deepspeed.init_inference(\n",
    "        model=getattr(pipe,\"model\", pipe),      # Transformers models\n",
    "        mp_size=1,        # Number of GPU\n",
    "        dtype=torch.float16, # dtype of the weights (fp16)\n",
    "        replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "        replace_with_kernel_inject=False, # replace the model with the kernel injector\n",
    "    )\n",
    "\n",
    "    print(\"!!!!DeepSpeed Inference Engine initialized!!!!!!!!\")\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    torch.cuda.synchronize(\"cuda\")\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "\n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    print(prompt)\n",
    "    # set valid HP for stable diffusion\n",
    "    num_inference_steps = data.pop(\"num_inference_steps\", 50)\n",
    "    guidance_scale = data.pop(\"guidance_scale\", 7.5)\n",
    "    num_images_per_prompt = data.pop(\"num_images_per_prompt\", 4)\n",
    "    width = data.pop(\"width\", 512)\n",
    "    height = data.pop(\"height\", 512)\n",
    "\n",
    "    # run generation with parameters\n",
    "    generated_images = pipe(\n",
    "        prompt,\n",
    "        #num_inference_steps=num_inference_steps,\n",
    "        #guidance_scale=guidance_scale,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_images_per_prompt=num_images_per_prompt\n",
    "    )[\"images\"]\n",
    "\n",
    "    # create response\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"generated_images\": encoded_images}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e314032",
   "metadata": {},
   "source": [
    "### Prepare pre-trained SD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff84d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import random\n",
    "\n",
    "HF_MODEL_ID=\"CompVis/stable-diffusion-v1-4\"\n",
    "HF_TOKEN=\"\" # your hf token: https://huggingface.co/settings/tokens\n",
    "assert len(HF_TOKEN) > 0, \"Please set HF_TOKEN to your huggingface token. You can find it here: https://huggingface.co/settings/tokens\"\n",
    "\n",
    "# download snapshot\n",
    "snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID,revision=\"fp16\",use_auth_token=HF_TOKEN)\n",
    "\n",
    "# create model dir\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "# copy snapshot to model dir\n",
    "copy_tree(snapshot_dir, str(model_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2447b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri=S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/stable-diffusion-v1-4\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdc3a1-c672-479c-9bbf-3dab1da01142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers[torch] in /opt/conda/lib/python3.8/site-packages (0.14.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (4.13.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (1.23.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (2022.10.31)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (9.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (0.10.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (2.28.2)\n",
      "Requirement already satisfied: accelerate>=0.11.0 in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (0.16.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from diffusers[torch]) (1.12.1+cu113)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate>=0.11.0->diffusers[torch]) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate>=0.11.0->diffusers[torch]) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate>=0.11.0->diffusers[torch]) (5.4.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.10.0->diffusers[torch]) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.10.0->diffusers[torch]) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata->diffusers[torch]) (3.13.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->diffusers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->diffusers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->diffusers[torch]) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->diffusers[torch]) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.26.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 ls s3://sagemaker-ap-southeast-1-687912291502/stable-diffusion/models/768-v-ema.ckpt\n",
    "#! aws s3 cp s3://sagemaker-ap-southeast-1-687912291502/stable-diffusion/models/768-v-ema.yaml ./models_ckpt/\n",
    "#!pip install diffusers==0.14.0\n",
    "#!cd /root/dreambooth/models_safetensor/ && wget https://huggingface.co/Lykon/DreamShaper/resolve/main/DreamShaper_3.3_baked_vae.safetensors\n",
    "#!python convert_original_stable_diffusion_to_diffusers.py  --checkpoint_path ./models_safetensor/DreamShaper_3.3_baked_vae.safetensors  --from_safetensor --dump_path ./models_diffuser\n",
    "#!python convert_original_stable_diffusion_to_diffusers.py  --checkpoint_path ./models_ckpt/768-v-ema.ckpt   --dump_path ./models_diffuser\n",
    "!pip3 install --upgrade diffusers[torch]\n",
    "!pip3 install transformers\n",
    "!pip3 install omegaconf\n",
    "!pip3 install safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c46a53-061b-4ca1-8a68-a1f752a6e615",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded: StableDiffusionPipeline {\n",
      "  \"_class_name\": \"StableDiffusionPipeline\",\n",
      "  \"_diffusers_version\": \"0.14.0\",\n",
      "  \"feature_extractor\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"requires_safety_checker\": false,\n",
      "  \"safety_checker\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"PNDMScheduler\"\n",
      "  ],\n",
      "  \"text_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"tokenizer\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNet2DConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKL\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "begin load deepspeed....\n",
      "[2023-03-08 10:44:01,507] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\n",
      "[2023-03-08 10:44:01,508] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2023-03-08 10:44:01,509] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-03-08 10:44:01,509] [INFO] [logging.py:77:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "deepspeed accelarate excpetion!\n",
      "'StableDiffusionPipeline' object has no attribute 'children'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import boto3\n",
    "import sagemaker\n",
    "import uuid\n",
    "import torch\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import traceback\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import deepspeed\n",
    "\n",
    "\n",
    "\n",
    "model_dir='/root/dreambooth/models_diffuser/'\n",
    "model = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16, revision=\"fp16\")\n",
    "print(\"model loaded:\",model)\n",
    " \n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"begin load deepspeed....\")    \n",
    "    model=deepspeed.init_inference(\n",
    "        model=getattr(model,\"model\", model),      # Transformers models\n",
    "        mp_size=1,        # Number of GPU\n",
    "        dtype=torch.float16, # dtype of the weights (fp16)\n",
    "        replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "        replace_with_kernel_inject=False, # replace the model with the kernel injector\n",
    "    )\n",
    "    print('model accelarate with deepspeed!')\n",
    "except Exception as e:\n",
    "    print(\"deepspeed accelarate excpetion!\")\n",
    "    print(e)\n",
    "    \n",
    "model = model.to(\"cuda\")\n",
    "model.enable_attention_slicing()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275cc61",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "#s3://sagemaker-us-east-1-549828897912/stable-diffusion-v1-4/model.tar.gz\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef7462",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988518bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "  base64_image = base64.b64decode(image_string)\n",
    "  buffer = BytesIO(base64_image)\n",
    "  return Image.open(buffer)\n",
    "\n",
    "# display PIL images as grid\n",
    "def display_images(images=None,columns=3, width=100, height=100):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "en = predictor.endpoint_name\n",
    "prompt = \"A dog trying catch a flying pizza art drawn by disney concept artists, golden colour, high quality, highly detailed, elegant, sharp focus\"\n",
    "prompt = \"portrait photo headshot by mucha, sharp focus, elegant, render, octane, detailed, award winning photography, masterpiece, rim lit\"\n",
    "prompt = \"priest, blue robes, 68 year old nun, national geographic, portrait, photo, photography\"\n",
    "prompt = \"hotel room with a swimming pool outside of the window, TV on the table, moon in the sky\"\n",
    "#prompt = \"那人却在灯火阑珊处，色彩艳丽，古风，资深插画师作品，桌面高清壁纸 Van Gogh style\"#3D绘画\n",
    "#prompt = \"interior design, open plan, kitchen and living room, modular furniture with cotton textiles, wooden floor, high ceiling, large steel windows viewing a city\"\n",
    "prompt = \"小桥流水人家，Van Gogh style\"\n",
    "# run prediction\n",
    "response = predictor.predict(data={\n",
    "  \"inputs\": prompt\n",
    "  }\n",
    ")\n",
    "\n",
    "# decode images\n",
    "decoded_images = [decode_base64_image(image) for image in response[\"generated_images\"]]\n",
    "\n",
    "# visualize generation\n",
    "display_images(decoded_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d8e26",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f93740",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47f38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
