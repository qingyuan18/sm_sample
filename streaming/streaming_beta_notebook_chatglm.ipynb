{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907ddafc",
   "metadata": {},
   "source": [
    "# Response streaming using LMI container on SageMaker\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it. \n",
    "We will use the SageMaker Runtime `InvokeEndpointWithResponseStream` API which returns model responses in a stream-like manner.\n",
    "\n",
    "Note that this API is currently in beta. DO NOT onboard any production use-cases to this API during this beta program.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b37f8d",
   "metadata": {},
   "source": [
    "## Step 1: Let's bump up SageMaker and import stuff\n",
    "\n",
    "The wheel installed here is a private preview wheel, you need to add into allowlist to run this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103773c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.144 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "jupyterlab-server 2.22.1 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker pip --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060d87df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Processing ./botocore-1.29.157-py3-none-any.whl\n",
      "Processing ./boto3-1.26.157-py3-none-any.whl\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.29.157)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore==1.29.157)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore==1.29.157)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0 (from boto3==1.26.157)\n",
      "  Using cached s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore==1.29.157)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.144\n",
      "    Uninstalling botocore-1.29.144:\n",
      "      Successfully uninstalled botocore-1.29.144\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.6.1\n",
      "    Uninstalling s3transfer-0.6.1:\n",
      "      Successfully uninstalled s3transfer-0.6.1\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.144\n",
      "    Uninstalling boto3-1.26.144:\n",
      "      Successfully uninstalled boto3-1.26.144\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.144 requires botocore==1.29.144, but you have botocore 1.29.157 which is incompatible.\n",
      "awscli 1.27.144 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "jupyterlab-server 2.22.1 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.157 botocore-1.29.157 jmespath-1.0.1 python-dateutil-2.8.2 s3transfer-0.6.1 six-1.16.0 urllib3-1.26.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Note the following may error depending on which awscli is installed in your jupyter kernel, \n",
    "# but that is ok \n",
    "\n",
    "%pip install botocore-*-py3-none-any.whl boto3-*-py3-none-any.whl --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b83d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://runtime.sagemaker-2017-05-13.normal.json --service-name sagemaker-runtime-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6245aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "boto3_session=boto3.session.Session(region_name=\"us-west-2\")\n",
    "smr = boto3.client('sagemaker-runtime-demo')\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a838bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: arn:aws:iam::687912291502:role/webui-notebook-stack-ExecutionRole-62U5FV4LJQS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557fedb",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef042d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.model_id=THUDM/chatglm-6b\n",
    "option.tensor_parallel_degree=1\n",
    "option.dtype=fp16\n",
    "option.enable_streaming=True\n",
    "gpu.maxWorkers=1\n",
    "option.predict_timeout=240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "665583f5-8515-4c6a-90c8-d630fbea0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers==4.27.1\n",
    "cpm_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c7ed8fd-159c-46dc-ba15-3e7af1a1b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from djl_python.streaming_utils import StreamingUtils\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "model = None\n",
    "tokenizer =None\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "    print(\"=================model_fn_Start=================\")\n",
    "    model_s3_path = os.environ['MODEL_S3_PATH']\n",
    "    print(\"=================model s3 path==================\"+model_s3_path)\n",
    "    os.system(\"sudo find / -name s5cmd\")\n",
    "    os.system(\"s5cmd sync {0} {1}\".format(model_s3_path+\"*\",\"/tmp/model/\"))\n",
    "    if os.environ[\"MODEL_TYPE\"] == \"ptuning\":\n",
    "        config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, pre_seq_len=128)\n",
    "        model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", config=config, trust_remote_code=True)\n",
    "        prefix_state_dict = torch.load(os.path.join(\"/tmp/model/\", \"pytorch_model.bin\"))\n",
    "        new_prefix_state_dict = {}\n",
    "        for k, v in prefix_state_dict.items():\n",
    "            if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "                new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "    elif os.environ[\"MODEL_TYPE\"] == \"full turning\":\n",
    "        print(\"====================load full turning=================\")\n",
    "        config = AutoConfig.from_pretrained(\"/tmp/model/\", trust_remote_code=True, pre_seq_len=128)\n",
    "        model = AutoModel.from_pretrained(\"/tmp/model/\", trust_remote_code=True)\n",
    "    else:\n",
    "        print(\"====================load normal ======================\")\n",
    "        config = AutoConfig.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "        model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "    #model = model.to(\"cuda\")\n",
    "    model = model.quantize(4)\n",
    "    model = model.half().cuda()\n",
    "    model = model.eval()\n",
    "    print(\"=================model_fn_End=================\")\n",
    "    return model,tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "    return model_fn(None)\n",
    "\n",
    "def stream(query, history):\n",
    "    if query is None or history is None:\n",
    "        yield {\"query\": \"\", \"response\": \"\", \"history\": [], \"finished\": True}\n",
    "    size = 0\n",
    "    response = \"\"\n",
    "    for response, history in model.stream_chat(tokenizer, query, history):\n",
    "        this_response = response[size:]\n",
    "        history = [list(h) for h in history]\n",
    "        size = len(response)\n",
    "        yield {\"delta\": this_response, \"response\": response, \"finished\": False}\n",
    "    print(\"Answer - {}\".format(response))\n",
    "    yield {\"query\": query, \"delta\": \"[EOS]\", \"response\": response, \"history\": history, \"finished\": True}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not model:\n",
    "        model,tokenizer = get_model(None)\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json()\n",
    "    outputs = Output()\n",
    "    outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "    outputs.add_stream_content(stream(data['query'],data['history']))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "507b5d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel/\n",
      "mymodel/serving.properties\n",
      "mymodel/requirements.txt\n",
      "mymodel/model.py\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "mv model.py mymodel/\n",
    "mv requirements.txt mymodel/\n",
    "# remove the following lines if not needed\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2282772",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57700799",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "Available framework are:\n",
    "- djl-deepspeed (0.20.0, 0.21.0)\n",
    "- djl-fastertransformer (0.21.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "654c48ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.22.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979f2c9",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49454131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-687912291502/large-model-lmi/code/mymodel.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "env = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "    #'MODEL_TYPE'                    : 'ptuning',\n",
    "    #'MODEL_TYPE'                    : 'full turning',\n",
    "    'MODEL_TYPE'                    : 'normal',\n",
    "    #'MODEL_S3_PATH'                 : 's3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/'\n",
    "    'MODEL_S3_PATH'                 : 's3://sagemaker-us-west-2-687912291502/llm/models/chatglm/simple/adgen-chatglm-6b-ft/'\n",
    "}\n",
    "\n",
    "model = Model(sagemaker_session=sess, image_uri=image_uri, model_data=code_artifact, env=env, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119f0cb",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a77095",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.4xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"streaming-test\")\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf7ca9",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f788c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "class StreamScanner:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        \n",
    "    def readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "995e2848",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop event ouput===\n",
      "line===\n",
      "{'delta': '', 'response': '', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '新加坡', 'response': '新加坡', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '是一个', 'response': '新加坡是一个', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '充满活力', 'response': '新加坡是一个充满活力', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '和', 'response': '新加坡是一个充满活力和', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '多元', 'response': '新加坡是一个充满活力和多元', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '文化', 'response': '新加坡是一个充满活力和多元文化', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '的城市', 'response': '新加坡是一个充满活力和多元文化的城市', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '国家', 'response': '新加坡是一个充满活力和多元文化的城市国家', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '，', 'response': '新加坡是一个充满活力和多元文化的城市国家，', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '有许多', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '好玩的', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '活动', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '和', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '景点', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '，', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '以下', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '是', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '一些', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '推荐', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '：', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '\\n\\n1', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：\\n\\n1', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '.', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：\\n\\n1.', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：\\n\\n1.', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': ' 滨海', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：\\n\\n1. 滨海', 'finished': False}loop event ouput===\n",
      "line===\n",
      "{'delta': '湾', 'response': '新加坡是一个充满活力和多元文化的城市国家，有许多好玩的活动和景点，以下是一些推荐：\\n\\n1. 滨海湾', 'finished': False}loop event ouput===\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 338 (char 337)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25404/3249682810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscanner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PayloadPart'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bytes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscanner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"line===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(resp.get(\"outputs\")[0], end='')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 338 (char 337)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "                \n",
    "body = {\"query\": \"新加坡有什么好玩的？\", \"history\":[],\"parameters\": {\"max_new_tokens\":128, \"enable_sampling\": \"true\"}}\n",
    "resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    print(\"loop event ouput===\")\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        resp = json.loads(line)\n",
    "        print(\"line===\")\n",
    "        #print(resp.get(\"outputs\")[0], end='')\n",
    "        print(resp.get(\"outputs\"), end='')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936c0d4",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9ebcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
